{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8356196b-3488-4237-80eb-94d278b8802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_39664\\3915360713.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_water_mgmt.loc[:, \"Indicator_Type\"] = \"Water Management\"\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_39664\\3915360713.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_water_mgmt.loc[:, \"Units\"] = \"PERCENT\"  # Assign default unit\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_39664\\3915360713.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_water_mgmt.fillna(0, inplace=True)  # Replace null values with 0\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_39664\\3915360713.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_unsentenced[\"Indicator_Type\"] = \"Unsentenced Detainees\"\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_39664\\3915360713.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_unsentenced[\"Units\"] = \"PERCENT\"  # Assign default unit\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_39664\\3915360713.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_homicide[\"Indicator_Type\"] = \"Homicide\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths for the datasets\n",
    "datasets = {\n",
    "    \"ER_H2O_IWRMD_FI\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/New Datasets/ER_H2O_IWRMD_FI.xlsx\",\n",
    "    \"GF_XPD_GBPC\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/New Datasets/GF_XPD_GBPC.xlsx\",\n",
    "    \"IU_COR_BRIB\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/New Datasets/IU_COR_BRIB.xlsx\",\n",
    "    \"VC_IHR_PSRC\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/New Datasets/VC_IHR_PSRC.xlsx\",\n",
    "    \"VC_PRS_UNSNT\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/New Datasets/VC_PRS_UNSNT.xlsx\",\n",
    "    \"VC_SNS_WALN_DRK\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/New Datasets/VC_SNS_WALN_DRK.xlsx\",\n",
    "    #\"GF_FRN_FDI\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/New Datasets/GF_FRN_FDI.xlsx\"\n",
    "}\n",
    "\n",
    "# Read datasets into DataFrames\n",
    "dataframes = {key: pd.read_excel(path) for key, path in datasets.items()}\n",
    "\n",
    "# Get available years dynamically from each dataset\n",
    "available_years = {key: [col for col in df.columns if col.isdigit()] for key, df in dataframes.items()}\n",
    "\n",
    "# Ensure required columns exist before selecting\n",
    "water_columns = [col for col in [\"GeoAreaName\", \"SeriesCode\", \"Indicator\"] + available_years[\"ER_H2O_IWRMD_FI\"] if col in dataframes[\"ER_H2O_IWRMD_FI\"].columns]\n",
    "df_water_mgmt = dataframes[\"ER_H2O_IWRMD_FI\"][water_columns] if water_columns else pd.DataFrame()\n",
    "\n",
    "# Standardize Indicator naming convention across datasets\n",
    "if not df_water_mgmt.empty:\n",
    "    df_water_mgmt.loc[:, \"Indicator\"] = \"6.5.1\"\n",
    "    df_water_mgmt.loc[:, \"Indicator_Type\"] = \"Water Management\"\n",
    "    df_water_mgmt.loc[:, \"Units\"] = \"PERCENT\"  # Assign default unit\n",
    "    df_water_mgmt.fillna(0, inplace=True)  # Replace null values with 0\n",
    "\n",
    "df_unsentenced = dataframes[\"VC_PRS_UNSNT\"][[col for col in [\"GeoAreaName\", \"SeriesCode\", \"Indicator\"] + available_years[\"VC_PRS_UNSNT\"] if col in dataframes[\"VC_PRS_UNSNT\"].columns]]\n",
    "df_unsentenced[\"Indicator_Type\"] = \"Unsentenced Detainees\"\n",
    "df_unsentenced[\"Units\"] = \"PERCENT\"  # Assign default unit\n",
    "\n",
    "df_homicide = dataframes[\"VC_IHR_PSRC\"][[col for col in [\"GeoAreaName\", \"SeriesCode\", \"Indicator\", \"Units\"] + available_years[\"VC_IHR_PSRC\"] if col in dataframes[\"VC_IHR_PSRC\"].columns]]\n",
    "df_homicide[\"Indicator_Type\"] = \"Homicide\"\n",
    "\n",
    "# Keep homicide values in original PER_100000_POP and retain the Units column\n",
    "def restructure_data(df):\n",
    "    \"\"\"Transforms the dataset to have years as columns and adds an Indicator column.\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    id_vars = [\"GeoAreaName\", \"SeriesCode\", \"Indicator\", \"Indicator_Type\", \"Units\"]\n",
    "    df_long = df.melt(id_vars=id_vars, var_name=\"Year\", value_name=\"Value\")\n",
    "    df_long[\"Value\"] = df_long[\"Value\"].fillna(0)  # Replace empty values with 0\n",
    "    return df_long\n",
    "\n",
    "# Transform individual datasets\n",
    "df_water_mgmt_long = restructure_data(df_water_mgmt)\n",
    "df_unsentenced_long = restructure_data(df_unsentenced)\n",
    "df_homicide_long = restructure_data(df_homicide)\n",
    "\n",
    "# Combine all indicators\n",
    "df_combined = pd.concat([df_water_mgmt_long, df_unsentenced_long, df_homicide_long], ignore_index=True)\n",
    "\n",
    "# Pivot the data to make years as columns\n",
    "index_cols = [\"GeoAreaName\", \"Indicator\", \"Indicator_Type\", \"SeriesCode\", \"Units\"]\n",
    "df_pivot = df_combined.pivot_table(index=index_cols, columns=\"Year\", values=\"Value\", aggfunc=\"sum\").reset_index()\n",
    "\n",
    "# Remove column name from pivot table\n",
    "df_pivot.columns.name = None\n",
    "\n",
    "# Rename GeoAreaName to Country\n",
    "df_pivot.rename(columns={\"GeoAreaName\": \"Country\"}, inplace=True)\n",
    "\n",
    "# Add SDG Goal column\n",
    "df_pivot[\"SDG_Goal\"] = \"16 & 6 (Peace, Justice, Water Management)\"\n",
    "\n",
    "df_pivot = df_pivot.fillna(0)\n",
    "\n",
    "# Save the result\n",
    "df_pivot.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9750414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted and integrated dataset saved to: C:/Users/rohit/Downloads/final_integrated_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_31552\\316943551.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat([output_df] + standardized_dfs, ignore_index=True)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the output.csv file to understand its structure\n",
    "output_csv_path = \"C:/Users/rohit/Downloads/output.csv\"\n",
    "output_df = pd.read_csv(output_csv_path)\n",
    "\n",
    "# Load the Excel files\n",
    "excel_files = {\n",
    "    \"IU_COR_BRIB\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/Newer Datasets/IU_COR_BRIB (1).xlsx\",\n",
    "    \"GF_XPD_GBPC\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/Newer Datasets/GF_XPD_GBPC (1).xlsx\",\n",
    "    \"EN_WWT_WWDS\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/Newer Datasets/EN_WWT_WWDS.xlsx\",\n",
    "    \"EN_H2O_OPAMBQ\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/Newer Datasets/EN_H2O_OPAMBQ.xlsx\",\n",
    "    \"DC_TOF_WASHL\": \"C:/Users/rohit/OneDrive/Documents/DAEN690/Newer Datasets/DC_TOF_WASHL.xlsx\"\n",
    "}\n",
    "\n",
    "# Read each sheet in the Excel files\n",
    "excel_data = {name: pd.ExcelFile(path) for name, path in excel_files.items()}\n",
    "\n",
    "# Define column mappings\n",
    "column_mappings = {\n",
    "    \"GeoAreaName\": \"Country\",\n",
    "    \"SeriesDescription\": \"Indicator_Type\",\n",
    "}\n",
    "\n",
    "# Standardize each dataset\n",
    "standardized_dfs = []\n",
    "for name, excel_file in excel_data.items():     \n",
    "    sheet_name = excel_file.sheet_names[0]  # Assume first sheet is relevant\n",
    "    df = excel_file.parse(sheet_name)  # Read first sheet\n",
    "    df = df.rename(columns=column_mappings)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    relevant_columns = [\"Country\", \"Indicator\", \"SeriesCode\", \"Indicator_Type\", \"Units\"] + \\\n",
    "                       [str(year) for year in range(2000, 2024)]\n",
    "    \n",
    "    # Ensure missing columns are added and in correct order\n",
    "    for col in relevant_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    df = df[relevant_columns]  # Reorder columns\n",
    "    standardized_dfs.append(df)\n",
    "\n",
    "# Concatenate all datasets into a single DataFrame\n",
    "merged_df = pd.concat([output_df] + standardized_dfs, ignore_index=True)\n",
    "\n",
    "# Fill missing values with 0\n",
    "merged_df = merged_df.fillna(0) \n",
    "\n",
    "# Remove 'SDG_Goal' column if it exists\n",
    "if 'SDG_Goal' in merged_df.columns:\n",
    "    merged_df = merged_df.drop(columns=['SDG_Goal'])\n",
    "\n",
    "# Sort the dataset by Country\n",
    "sorted_final_df = merged_df.sort_values(by=[\"Country\"]).reset_index(drop=True)\n",
    "\n",
    "# Save the sorted dataset\n",
    "sorted_output_path = \"C:/Users/rohit/Downloads/final_integrated_output.csv\"\n",
    "sorted_final_df.to_csv(sorted_output_path, index=False)\n",
    "\n",
    "print(f\"Sorted and integrated dataset saved to: {sorted_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Country Indicator Indicator_Type   SeriesCode           Units  \\\n",
      "0          Afghanistan    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "1              Albania    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "2              Algeria    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "3       American Samoa    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "4              Andorra    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "5               Angola    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "6             Anguilla    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "7  Antigua and Barbuda    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "8            Argentina    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "9              Armenia    16.1.1       Homicide  VC_IHR_PSRC  PER_100000_POP   \n",
      "\n",
      "    2000   2001   2002   2003   2004  ...   2014   2015   2016   2017   2018  \\\n",
      "0   0.00   0.00   0.00   0.00   0.00  ...   0.00   9.98   6.69   6.80   8.18   \n",
      "1  12.41  21.01  20.83  16.17  12.87  ...  13.91   6.65   8.21   6.04   6.87   \n",
      "2   0.00   0.00   0.00   0.00   0.00  ...   4.43   4.11   3.99   3.78   4.03   \n",
      "3   0.00   1.71  12.03   5.18   6.94  ...   5.75   7.79   0.00  42.48  80.69   \n",
      "4   0.00   0.00   0.00   0.00   1.30  ...   0.00   0.00   0.00   0.00   0.00   \n",
      "5   0.00   0.00   0.00   0.00   0.00  ...   0.00   4.46   4.10   0.00   0.00   \n",
      "6  27.42   0.00  17.30  17.03   8.38  ...  28.33   0.00   0.00   0.00   0.00   \n",
      "7   6.66  28.48  19.87   6.40   5.07  ...   0.00  11.45  20.43  67.57  39.95   \n",
      "8   0.00   8.35   9.42   7.75   6.05  ...   7.54  19.78   6.01  15.84  16.18   \n",
      "9   8.77   8.63   6.79   5.16   8.31  ...   7.70   8.06   9.23   7.72   5.51   \n",
      "\n",
      "    2019   2020   2021   2022  2023  \n",
      "0   8.45   6.59   4.02   0.00   0.0  \n",
      "1   6.78   6.39   6.94   4.96   0.0  \n",
      "2   3.62   4.48   4.69   5.27   0.0  \n",
      "3   0.00   0.00   0.00   0.00   0.0  \n",
      "4   0.00   7.62   0.00   0.00   0.0  \n",
      "5   0.00   0.00   0.00   0.00   0.0  \n",
      "6   0.00   0.00   0.00   0.00   0.0  \n",
      "7   9.89  29.86  52.14  32.81   0.0  \n",
      "8  15.53  16.18  13.94  12.98   0.0  \n",
      "9   8.18   5.73   6.84   0.00   0.0  \n",
      "\n",
      "[10 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the original SDG 6 and 16 dataset (already in wide format)\n",
    "sorted_df = pd.read_csv(\"C:/Users/rohit/OneDrive/Documents/DAEN690/Dataset_Sorted_by_Indicator.csv\")\n",
    "\n",
    "# Step 2: Load SDG 17 dataset (raw format)\n",
    "sdg17_df = pd.read_excel(\"C:/Users/rohit/OneDrive/Documents/DAEN690/Newer Datasets/GF_FRN_FDI.xlsx\")\n",
    "\n",
    "# Step 3: Melt SDG 17 into long format\n",
    "sdg17_long = sdg17_df.melt(\n",
    "    id_vars=[\"Goal\", \"Target\", \"Indicator\", \"SeriesCode\", \"SeriesDescription\", \n",
    "             \"GeoAreaCode\", \"GeoAreaName\", \"Reporting Type\", \"Units\"],\n",
    "    var_name=\"TimePeriod\",\n",
    "    value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Step 4: Pivot SDG 17 back into wide format with year columns\n",
    "sdg17_pivot = sdg17_long.pivot_table(\n",
    "    index=[\"GeoAreaName\", \"Indicator\", \"SeriesCode\", \"Units\"],\n",
    "    columns=\"TimePeriod\",\n",
    "    values=\"Value\"\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Rename and format to match Dataset_Sorted_by_Indicator.csv\n",
    "sdg17_pivot = sdg17_pivot.rename(columns={\"GeoAreaName\": \"Country\"})\n",
    "sdg17_pivot[\"Indicator_Type\"] = \"Quantitative\"  # or any default as needed\n",
    "\n",
    "# Step 6: Ensure column consistency\n",
    "expected_columns = ['Country', 'Indicator', 'Indicator_Type', 'SeriesCode', 'Units'] + \\\n",
    "                   [str(year) for year in range(2000, 2024)]\n",
    "\n",
    "# Add missing columns (years) with NA\n",
    "for col in expected_columns:\n",
    "    if col not in sdg17_pivot.columns:\n",
    "        sdg17_pivot[col] = pd.NA\n",
    "\n",
    "# Reorder columns\n",
    "sdg17_pivot = sdg17_pivot[expected_columns]\n",
    "\n",
    "# Step 7: Concatenate SDG 17 to the original dataset\n",
    "combined_dataset = pd.concat([sorted_df, sdg17_pivot], ignore_index=True)\n",
    "\n",
    "# Step 8: Final cleanup â€” reset index and sort by Indicator and Country\n",
    "final_dataset = combined_dataset.reset_index(drop=True)\n",
    "final_dataset_sorted = final_dataset.sort_values(by=[\"Indicator\", \"Country\"]).reset_index(drop=True)\n",
    "\n",
    "# Output: final_dataset_sorted is your complete clean dataset\n",
    "print(final_dataset_sorted.head(10))\n",
    "\n",
    "# Save the sorted dataset\n",
    "sorted_output_path = \"C:/Users/rohit/OneDrive/Documents/DAEN690/final_dataset_sdg61617\" \\\n",
    "\".csv\"\n",
    "sorted_final_df.to_csv(sorted_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e6984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
